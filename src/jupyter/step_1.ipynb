{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import phonenumbers as ph\n",
    "import pycountry as pc\n",
    "from fuzzywuzzy import fuzz\n",
    "import regex\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data into pandas dataframes\n",
    "fb_df = pd.read_csv('../../datasets/input/facebook_dataset.csv', delimiter=\",\", quotechar='\"', escapechar='\\\\' ,dtype=str)\n",
    "g_df = pd.read_csv('../../datasets/input/google_dataset.csv', delimiter=\",\", quotechar='\"', escapechar='\\\\' ,dtype=str)\n",
    "w_df = pd.read_csv('../../datasets/input/website_dataset.csv', delimiter=\";\", quotechar='\"',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling in null values\n",
    "fb_df = fb_df.replace(['NaN','nan','NAN'], None)\n",
    "g_df = g_df.replace(['NaN','nan','NAN'], None)\n",
    "w_df = w_df.replace(['NaN','nan','NAN'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns fb-google  : {'address', 'phone', 'domain', 'country_code', 'name', 'country_name', 'region_code', 'zip_code', 'identifier', 'city', 'region_name', 'phone_country_code'}\n",
      "Common columns fb-web     : {'phone', 'identifier'}\n",
      "Common columns google-web : {'phone', 'identifier'}\n",
      "Common columns all        : {'phone', 'identifier'}\n"
     ]
    }
   ],
   "source": [
    "#Creating an identifier column to let-us know the source of the data\n",
    "fb_df['identifier'] = fb_df.index.map(lambda x: str(x) + '-facebook') \n",
    "g_df['identifier'] = g_df.index.map(lambda x: str(x) + '-google')\n",
    "w_df['identifier'] = w_df.index.map(lambda x: str(x) + '-website')\n",
    "\n",
    "fb_col_dy = set(fb_df.columns)\n",
    "g_col_dy = set(g_df.columns)\n",
    "w_col_dy = set(w_df.columns)\n",
    "\n",
    "common_columns_fb_g = fb_col_dy.intersection(g_col_dy)\n",
    "common_columns_fb_w = fb_col_dy.intersection(w_col_dy)\n",
    "common_columns_g_w = g_col_dy.intersection(w_col_dy)\n",
    "common_columns_all =  fb_col_dy.intersection(g_col_dy).intersection(w_col_dy)\n",
    "\n",
    "print(f'Common columns fb-google  : {common_columns_fb_g}')\n",
    "print(f'Common columns fb-web     : {common_columns_fb_w}')\n",
    "print(f'Common columns google-web : {common_columns_g_w}')\n",
    "print(f'Common columns all        : {common_columns_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns to be the same where there is same content\n",
    "w_df.rename(columns={'root_domain':'domain',\n",
    "                    'main_city':'city',\n",
    "                    'main_country':'country_name',\n",
    "                    'main_region':'region',\n",
    "                    's_category':'category'}, inplace=True)\n",
    "\n",
    "# g_df.rename(columns={'raw_address':'address'\n",
    "#                     }, inplace=True)\n",
    "\n",
    "fb_df.rename(columns={'categories':'category'\n",
    "                    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns fb-google  : {'address', 'phone', 'domain', 'country_code', 'name', 'country_name', 'category', 'region_code', 'zip_code', 'identifier', 'city', 'region_name', 'phone_country_code'}\n",
      "Common columns fb-web     : {'phone', 'domain', 'country_name', 'category', 'identifier', 'city'}\n",
      "Common columns google-web : {'phone', 'domain', 'country_name', 'category', 'identifier', 'city'}\n",
      "Common columns all        : {'phone', 'domain', 'country_name', 'category', 'identifier', 'city'}\n"
     ]
    }
   ],
   "source": [
    "fb_col_dy = set(fb_df.columns)\n",
    "g_col_dy = set(g_df.columns)\n",
    "w_col_dy = set(w_df.columns)\n",
    "\n",
    "common_columns_fb_g = fb_col_dy.intersection(g_col_dy)\n",
    "common_columns_fb_w = fb_col_dy.intersection(w_col_dy)\n",
    "common_columns_g_w = g_col_dy.intersection(w_col_dy)\n",
    "common_columns_all =  fb_col_dy.intersection(g_col_dy).intersection(w_col_dy)\n",
    "\n",
    "print(f'Common columns fb-google  : {common_columns_fb_g}')\n",
    "print(f'Common columns fb-web     : {common_columns_fb_w}')\n",
    "print(f'Common columns google-web : {common_columns_g_w}')\n",
    "print(f'Common columns all        : {common_columns_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format and validate phone numbers\n",
    "def format_and_validate_phone_number(phone_number):\n",
    "    try:\n",
    "        if not phone_number.startswith('+'):\n",
    "            phone_number = \"+\"+phone_number    \n",
    "        parsed_number = ph.parse(phone_number, None)\n",
    "        formatted_number = ph.format_number(parsed_number, ph.PhoneNumberFormat.INTERNATIONAL)\n",
    "        return formatted_number if ph.is_valid_number(parsed_number) else None\n",
    "    except ph.phonenumberutil.NumberParseException:\n",
    "        return None\n",
    "    \n",
    "def parse_country_name_to_code(country_name):\n",
    "    try:\n",
    "        country = pc.countries.get(name=country_name)\n",
    "        if country is not None:\n",
    "            return str.lower(country.alpha_2)\n",
    "        else:\n",
    "            return None\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "def normalise_comapny_name(company_name):\n",
    "    # Define a list of company types to remove\n",
    "    company_types = ['Ltd', 'Inc', 'Corp', 'LLC', 'Ltd.', 'Inc.', 'Corp.', 'LLC.', 'Co', 'AG', 'PLC', 'SA', 'SRL', 'NV', 'Pty', 'AB', 'BV', 'MD']\n",
    "\n",
    "   # Remove special characters from the company name\n",
    "    company_name = regex.sub(r'\\p{P}', '', company_name) #removing special characters\n",
    "    company_name = regex.sub(r'\\+[0-9]+|\\<|\\>|Â°|^[0-9]+$', '',company_name) # removingspecial characters not removed previously\n",
    "    company_name = regex.sub(r'[^\\w\\s]', '', company_name) # removing emoticons\n",
    "\n",
    "    # Use regular expression to match and remove company types\n",
    "    pattern = regex.compile(r'\\b(?:{})\\b'.format('|'.join(map(regex.escape, company_types))), regex.IGNORECASE)\n",
    "    company_name = pattern.sub('', company_name)\n",
    "\n",
    "    # Remove leading/trailing whitespaces and reduce multiple spaces to a single space\n",
    "    company_name = regex.sub(r'\\s+', ' ', company_name).strip()\n",
    "    if (str.upper(company_name) == 'NAN' or company_name == ''):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return str.upper(company_name)\n",
    "\n",
    "def clean_city(string):\n",
    "    if pd.isnull(string):\n",
    "        return None\n",
    "    elif isinstance(string, str):\n",
    "        string = regex.sub( r\"[^\\w\\s]\", '', string)  # Remove special characters\n",
    "        string = string.strip()  # Remove trailing and leading spaces\n",
    "        return string\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def create_json_address(df):\n",
    "    json_addresses = []\n",
    "    unique_addresses = set()\n",
    "    for _, row in df.iterrows():\n",
    "        address = {\n",
    "            'id': row['identfier'],\n",
    "            'phone': row['phone_parsed'],\n",
    "            # 'country': row['country_code'],\n",
    "            'city': row['city'],\n",
    "            'zip_code': row['zip_code'],\n",
    "            'raw_address': row['address']\n",
    "        }\n",
    "        address_str = json.dumps(address)\n",
    "        if address_str not in unique_addresses:\n",
    "            json_addresses.append(address)\n",
    "            unique_addresses.add(address_str)\n",
    "        \n",
    "    return json.dumps(json_addresses)\n",
    "\n",
    "def calculate_company_name_fuzzy_score(row):\n",
    "    company_name_x = str(row['company_name_norm_x'])  # Convert to string\n",
    "    company_name_y = str(row['company_name_norm_y'])  # Convert to string\n",
    "    \n",
    "    # Handle NaN values\n",
    "    if company_name_x == 'nan' or company_name_y == 'nan':\n",
    "        return 0\n",
    "    \n",
    "    return fuzz.ratio(company_name_x, company_name_y)\n",
    "\n",
    "def check_category_inclusion(row):\n",
    "    result = np.nan\n",
    "    category_x = str(row['category_x']) # Convert to string\n",
    "    category_y = str(row['category_y']) # Convert to string\n",
    "    if (category_x == 'nan' and category_y == 'nan') or (category_x != 'nan' and category_y == 'nan'):\n",
    "        result = 0\n",
    "    else:\n",
    "        if category_x == 'nan' and category_y != 'nan':\n",
    "            result = 1\n",
    "        else:\n",
    "            if category_y in category_x:\n",
    "                result = 0\n",
    "            else:\n",
    "                result = 1\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the data types\n",
    "\n",
    "fb_df = fb_df.astype(str)\n",
    "g_df = g_df.astype(str)\n",
    "w_df = w_df.astype(str)\n",
    "\n",
    "#Checking domain colums to see if it contains a valid domain string\n",
    "# Regex pattern for domain validation\n",
    "regex_pattern = r'^[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "fb_df['valid_domain'] = np.where(fb_df['domain'].str.match(regex_pattern),0,1) # 0 - true, 1 - false\n",
    "g_df['valid_domain'] = np.where(g_df['domain'].str.match(regex_pattern),0,1) # 0 - true, 1 - false\n",
    "w_df['valid_domain'] = np.where(w_df['domain'].str.match(regex_pattern),0,1) # 0 - true, 1 - false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72009\n",
       "1        1\n",
       "Name: valid_domain, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_df['valid_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    356519\n",
       "1         1\n",
       "Name: valid_domain, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_df['valid_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72004\n",
       "1       14\n",
       "Name: valid_domain, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df['valid_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking phone-numbers\n",
    "fb_df['phone_parsed'] = fb_df['phone'].apply(format_and_validate_phone_number)\n",
    "g_df['phone_parsed'] = g_df['phone'].apply(format_and_validate_phone_number)\n",
    "w_df['phone_parsed'] = w_df['phone'].apply(format_and_validate_phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising Company Names\n",
    "fb_df['company_name_norm'] = fb_df['name'].apply(normalise_comapny_name)\n",
    "g_df['company_name_norm'] = g_df['name'].apply(normalise_comapny_name)\n",
    "w_df['company_name_norm'] = w_df['legal_name'].apply(normalise_comapny_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting country code for websites df\n",
    "w_df['country_code'] = w_df['country_name'].apply(parse_country_name_to_code)\n",
    "\n",
    "#adding missing columns with null for the websites df\n",
    "w_df['address'] = None\n",
    "w_df['zip_code'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commnon columns: Index(['domain', 'address', 'category', 'city', 'country_code', 'country_name',\n",
      "       'phone', 'zip_code', 'identifier', 'valid_domain', 'phone_parsed',\n",
      "       'company_name_norm'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Checking the common columns \n",
    "common_columns_all =  fb_df.columns.intersection(g_df.columns).intersection(w_df.columns)\n",
    "\n",
    "print(f'Commnon columns: {common_columns_all}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Normalised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_columns = ['identifier','company_name_norm','country_code','phone_parsed','domain','city','zip_code','category','address']\n",
    "\n",
    "fb_norm_df = fb_df[fb_df['valid_domain']==0][normalised_columns].copy()\n",
    "fb_norm_df = fb_norm_df.replace(['NaN','nan','NAN'], None)\n",
    "\n",
    "g_norm_df = g_df[g_df['valid_domain']==0][normalised_columns].copy()\n",
    "g_norm_df = g_norm_df.replace(['NaN','nan','NAN'], None)\n",
    "\n",
    "w_norm_df = w_df[w_df['valid_domain']==0][normalised_columns].copy()\n",
    "w_norm_df = w_norm_df.replace(['NaN','nan','NAN'], None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_norm_df.to_parquet('../../datasets/working/fb_norm.parquet')\n",
    "g_norm_df.to_parquet('../..datasets/working/g_norm.parquet')\n",
    "w_norm_df.to_parquet('../..datasets/working/w_norm.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all data\n",
    "- Merging data into one big dataset to start enriching and deduplication\n",
    "- Ideally would be good to merge on infomation that is consistent, like domain, zip code, phone number, or combinations between those.\n",
    "- Mergin on domain is only possible between facebook and webistes datasets as google has common social media or industry platform domains.\n",
    "- Before mergin some more data cleanup will be done in order to maximize the joining success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facebook.com                      71983\n",
       "postoffice.co.uk                   6010\n",
       "instagram.com                      5647\n",
       "ihg.com                            4356\n",
       "marriott.com                       3979\n",
       "                                  ...  \n",
       "streatsideeatery.com                  1\n",
       "slsi.ca                               1\n",
       "maplemoonwebdesign.com                1\n",
       "dentalperfections.co.uk               1\n",
       "aspenapplianceandfurniture.com        1\n",
       "Name: domain, Length: 72009, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_norm_df['domain'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websites company name consistency: \n",
      "\t - Null Values    :  40035 [ 55.6 %] \n",
      "\t - Not Null Values:  31969 [ 44.4 %]\n",
      "Facebook company name consistency: \n",
      "\t - Null Values    :      3 [ 0.0 %] \n",
      "\t - Not Null Values:  72006 [ 100.0 %]\n",
      "Google company name consistency  : \n",
      "\t - Null Values    :     41 [ 0.01 %] \n",
      "\t - Not Null Values: 356478 [ 99.99 %]\n"
     ]
    }
   ],
   "source": [
    "null_mask = w_norm_df['company_name_norm'].isnull()\n",
    "w_total = len(w_norm_df)\n",
    "w_null_company = len(w_norm_df[null_mask])\n",
    "w_nnull_company = len(w_norm_df[~null_mask])\n",
    "\n",
    "null_mask = g_norm_df['company_name_norm'].isnull()\n",
    "g_total = len(g_norm_df)\n",
    "g_null_company = len(g_norm_df[null_mask])\n",
    "g_nnull_company = len(g_norm_df[~null_mask])\n",
    "\n",
    "null_mask = fb_norm_df['company_name_norm'].isnull()\n",
    "fb_total = len(fb_norm_df)\n",
    "fb_null_company = len(fb_norm_df[null_mask])\n",
    "fb_nnull_company = len(fb_norm_df[~null_mask])\n",
    "\n",
    "print(f'Websites company name consistency: \\n\\t - Null Values    :  {w_null_company} [ {round((w_null_company/w_total)*100,2)} %] \\n\\t - Not Null Values:  {w_nnull_company} [ {round((w_nnull_company/w_total)*100,2)} %]')\n",
    "print(f'Facebook company name consistency: \\n\\t - Null Values    :      {fb_null_company} [ {round((fb_null_company/fb_total)*100,2)} %] \\n\\t - Not Null Values:  {fb_nnull_company} [ {round((fb_nnull_company/fb_total)*100,2)} %]')\n",
    "print(f'Google company name consistency  : \\n\\t - Null Values    :     {g_null_company} [ {round((g_null_company/g_total)*100,2)} %] \\n\\t - Not Null Values: {g_nnull_company} [ {round((g_nnull_company/g_total)*100,2)} %]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460453, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting all companies we have a name for\n",
    "c_fb_norm_df = fb_norm_df[fb_norm_df['company_name_norm'].notnull()].copy()\n",
    "c_fb_norm_df = c_fb_norm_df.reset_index(drop=True)\n",
    "\n",
    "c_g_norm_df = g_norm_df[g_norm_df['company_name_norm'].notnull()].copy()\n",
    "c_g_norm_df = c_g_norm_df.reset_index(drop=True)\n",
    "\n",
    "c_w_norm_df = w_norm_df[w_norm_df['company_name_norm'].notnull()].copy()\n",
    "c_w_norm_df = c_w_norm_df.reset_index(drop=True)\n",
    "\n",
    "full_df = pd.concat( [c_fb_norm_df, c_g_norm_df ,c_w_norm_df], ignore_index=True)\n",
    "full_df = full_df.replace(['NaN','nan','NAN'],None)\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking city for special characters\n",
    "full_df['city'] = full_df['city'].apply(clean_city)\n",
    "full_df = full_df.replace(['NaN','nan','NAN'],None)\n",
    "full_df.to_parquet('../../datasets/working/full_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records                 : 460453    [100.00 %]\n",
      "Total records no country      :  68954    [ 14.98 %]\n",
      "Total records no domain       :      0    [  0.0  %]\n",
      "Total records no city         :  75508    [  16.4 %]\n",
      "Total records no zip          : 151015    [  32.8 %]\n",
      "Total records no phone_parsed :  63640    [ 13.82 %]\n",
      "Total records no category     :  68503    [ 14.88 %]\n",
      "Total records no address      :  72535    [ 15.75 %]\n"
     ]
    }
   ],
   "source": [
    "total_records = len(full_df)\n",
    "null_country  = len(full_df[full_df['country_code'].isnull()])\n",
    "null_domain  = len(full_df[full_df['domain'].isnull()])\n",
    "null_city = len(full_df[full_df['city'].isnull()])\n",
    "null_zip = len(full_df[full_df['zip_code'].isnull()])\n",
    "null_phones = len(full_df[full_df['phone_parsed'].isnull()])\n",
    "null_category = len(full_df[full_df['category'].isnull()])\n",
    "null_address = len(full_df[full_df['address'].isnull()])\n",
    "\n",
    "print(f'Total records                 : {total_records}    [100.00 %]')\n",
    "print(f'Total records no country      :  {null_country}    [ {round((null_country/total_records)*100,2)} %]')\n",
    "print(f'Total records no domain       :      {null_domain}    [  {round((null_domain/total_records)*100,2)}  %]')\n",
    "print(f'Total records no city         :  {null_city}    [  {round((null_city/total_records)*100,2)} %]')\n",
    "print(f'Total records no zip          : {null_zip}    [  {round((null_zip/total_records)*100,2)} %]')\n",
    "print(f'Total records no phone_parsed :  {null_phones}    [ {round((null_phones/total_records)*100,2)} %]')\n",
    "print(f'Total records no category     :  {null_category}    [ {round((null_category/total_records)*100,2)} %]')\n",
    "print(f'Total records no address      :  {null_address}    [ {round((null_address/total_records)*100,2)} %]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records                 : 356519    [100.00 %]\n",
      "Total records no country      :  52435    [ 14.71 %]\n",
      "Total records no domain       :      0    [  0.0  %]\n",
      "Total records no city         :  45518    [ 12.77 %]\n",
      "Total records no zip          :  83120    [ 23.31 %]\n",
      "Total records no phone_parsed :  32685    [  9.17 %]\n",
      "Total records no category     :  51650    [ 14.49 %]\n",
      "Total records no address      :  25968    [  7.28 %]\n",
      "Total records no address      :     41    [  0.01 %]\n"
     ]
    }
   ],
   "source": [
    "#Trying to understand the google dataset\n",
    "total_records = len(g_norm_df)\n",
    "null_country  = len(g_norm_df[g_norm_df['country_code'].isnull()])\n",
    "null_domain  = len(g_norm_df[g_norm_df['domain'].isnull()])\n",
    "null_city = len(g_norm_df[g_norm_df['city'].isnull()])\n",
    "null_zip = len(g_norm_df[g_norm_df['zip_code'].isnull()])\n",
    "null_phones = len(g_norm_df[g_norm_df['phone_parsed'].isnull()])\n",
    "null_category = len(g_norm_df[g_norm_df['category'].isnull()])\n",
    "null_address = len(g_norm_df[g_norm_df['address'].isnull()])\n",
    "null_company_name = len(g_norm_df[g_norm_df['company_name_norm'].isnull()])\n",
    "\n",
    "print(f'Total records                 : {total_records}    [100.00 %]')\n",
    "print(f'Total records no country      :  {null_country}    [ {round((null_country/total_records)*100,2)} %]')\n",
    "print(f'Total records no domain       :      {null_domain}    [  {round((null_domain/total_records)*100,2)}  %]')\n",
    "print(f'Total records no city         :  {null_city}    [ {round((null_city/total_records)*100,2)} %]')\n",
    "print(f'Total records no zip          :  {null_zip}    [ {round((null_zip/total_records)*100,2)} %]')\n",
    "print(f'Total records no phone_parsed :  {null_phones}    [  {round((null_phones/total_records)*100,2)} %]')\n",
    "print(f'Total records no category     :  {null_category}    [ {round((null_category/total_records)*100,2)} %]')\n",
    "print(f'Total records no address      :  {null_address}    [  {round((null_address/total_records)*100,2)} %]')\n",
    "print(f'Total records no address      :     {null_company_name}    [  {round((null_company_name/total_records)*100,2)} %]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 1.8 s, total: 3min 2s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agg_data = full_df.groupby(['company_name_norm','country_code','domain'])\\\n",
    "                     .apply(lambda df: pd.Series({\n",
    "                                          'rows_count': len(df),\n",
    "                                          'categories': '|'.join(df['category'].unique()),\n",
    "                                          'addresses': create_json_address(df)\n",
    "                                                        })).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_by_domain = w_df.groupby(['root_domain'])\\\n",
    "                .apply(lambda df: pd.Series({\n",
    "                            'rows_count': len(df),\n",
    "                            'legal_names': '||'.join(df['legal_name'].unique()),\n",
    "                            'countries': '||'.join(df['main_country'].unique())\n",
    "                                            })).reset_index()\n",
    "\n",
    "w_multiple_companies = w_by_domain[w_by_domain['rows_count']>1]\n",
    "w_single_companies = w_by_domain[w_by_domain['rows_count']==1]\n",
    "\n",
    "print(f'w_multiple_companies: {w_multiple_companies}')\n",
    "print(f'w_single_companies: {w_single_companies}')\n",
    "# to normalise:\n",
    "# - main_country\n",
    "# - main_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_multiple_companies: domain         15291\n",
      "rows_count     15291\n",
      "legal_names    15291\n",
      "countries      15291\n",
      "dtype: int64\n",
      "g_single_companies: domain         56719\n",
      "rows_count     56719\n",
      "legal_names    56719\n",
      "countries      56719\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "g_by_domain = g_df.groupby(['domain'])\\\n",
    "                .apply(lambda df: pd.Series({\n",
    "                            'rows_count': len(df),\n",
    "                            'legal_names': '||'.join(df['name'].unique()),\n",
    "                            'countries': '||'.join(df['country_code'].unique())\n",
    "                                            })).reset_index()\n",
    "\n",
    "g_multiple_companies = g_by_domain[g_by_domain['rows_count']>1].count()\n",
    "g_single_companies = g_by_domain[g_by_domain['rows_count']==1].count()\n",
    "\n",
    "print(f'g_multiple_companies: {g_multiple_companies}')\n",
    "print(f'g_single_companies: {g_single_companies}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
